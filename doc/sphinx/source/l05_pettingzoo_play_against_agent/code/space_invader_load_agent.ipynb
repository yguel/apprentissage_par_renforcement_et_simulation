{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48cbe9-3bce-482e-a982-39b66a701b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "import torch\n",
    "from pettingzoo.atari import space_invaders_v2\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from agilerl.algorithms.maddpg import MADDPG\n",
    "\n",
    "\n",
    "# Define function to return image\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configure the environment\n",
    "    env = space_invaders_v2.parallel_env(render_mode=\"rgb_array\")\n",
    "    channels_last = True  # Needed for environments that use images as observations\n",
    "    if channels_last:\n",
    "        # Environment processing for image based observations\n",
    "        env = ss.frame_skip_v0(env, 4)\n",
    "        env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1)\n",
    "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "        env = ss.frame_stack_v1(env, 4)\n",
    "    env.reset()\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        discrete_actions = True\n",
    "        max_action = None\n",
    "        min_action = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        discrete_actions = False\n",
    "        max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "        min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Pre-process image dimensions for pytorch convolutional layers\n",
    "    if channels_last:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Load the saved agent\n",
    "    path = \"./models/MADDPG/MADDPG_space_invader_trained_agent.pt\"\n",
    "    maddpg = MADDPG.load(path, device)\n",
    "\n",
    "    # Define test loop parameters\n",
    "    episodes = 10  # Number of episodes to test agent on\n",
    "    max_steps = 500  # Max number of steps to take in the environment in each episode\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    frames = []  # List to collect frames\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "\n",
    "    # Test loop for inference\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            if channels_last:\n",
    "                state = {\n",
    "                    agent_id: np.moveaxis(np.expand_dims(s, 0), [3], [1])\n",
    "                    for agent_id, s in state.items()\n",
    "                }\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = maddpg.get_action(\n",
    "                state, training=False, infos=info\n",
    "            )\n",
    "            if maddpg.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Save the frame for this step and append to frames list\n",
    "            frame = env.render()\n",
    "            frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(\n",
    "                {agent: a.squeeze() for agent, a in action.items()}\n",
    "            )\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward for each agent\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "    env.close()\n",
    "\n",
    "    # Save the gif to specified path\n",
    "    gif_path = \"./videos/\"\n",
    "    os.makedirs(gif_path, exist_ok=True)\n",
    "    imageio.mimwrite(\n",
    "        os.path.join(\"./videos/\", \"space_invaders.gif\"), frames, duration=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73104ad1-60f3-470e-bc75-22c65c1f3277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
