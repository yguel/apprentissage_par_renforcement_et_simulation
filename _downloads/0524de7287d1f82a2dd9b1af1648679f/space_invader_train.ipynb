{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08d2a91-88d5-433c-aa45-5dcd0d974079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████▌                                                                                  | 10000/20000 [04:40<04:23, 38.02step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global steps 10000 ---\n",
      "Steps 10000\n",
      "Scores:\n",
      "    first_0 score: 1.7\n",
      "    second_0 score: 1.5\n",
      "Fitness\n",
      "    first_0 fitness: 3.0\n",
      "    second_0 fitness: 17.0\n",
      "Previous 5 fitness avgs\n",
      "  first_0 fitness average: 3.0\n",
      "  second_0 fitness average: 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [11:37<00:00, 28.67step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global steps 20000 ---\n",
      "Steps 20000\n",
      "Scores:\n",
      "    first_0 score: 7.5\n",
      "    second_0 score: 6.888888888888889\n",
      "Fitness\n",
      "    first_0 fitness: 3.0\n",
      "    second_0 fitness: 17.0\n",
      "Previous 5 fitness avgs\n",
      "  first_0 fitness average: 3.0\n",
      "  second_0 fitness average: 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This tutorial shows how to train an MADDPG agent on the space invaders atari environment.\n",
    "\n",
    "Authors: Michael (https://github.com/mikepratt1), Nick (https://github.com/nicku-a)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "import torch\n",
    "from pettingzoo.atari import space_invaders_v2\n",
    "from tqdm import trange\n",
    "\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.utils.utils import create_population\n",
    "from agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define the network configuration\n",
    "    NET_CONFIG = {\n",
    "        \"arch\": \"cnn\",  # Network architecture\n",
    "        \"hidden_size\": [32, 32],  # Network hidden size\n",
    "        \"channel_size\": [32, 32],  # CNN channel size\n",
    "        \"kernel_size\": [3, 3],  # CNN kernel size\n",
    "        \"stride_size\": [2, 2],  # CNN stride size\n",
    "        \"normalize\": True,  # Normalize image from range [0,255] to [0,1]\n",
    "    }\n",
    "\n",
    "    # Define the initial hyperparameters\n",
    "    INIT_HP = {\n",
    "        \"POPULATION_SIZE\": 1,\n",
    "        \"ALGO\": \"MADDPG\",  # Algorithm\n",
    "        # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "        \"CHANNELS_LAST\": True,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"O_U_NOISE\": True,  # Ornstein Uhlenbeck action noise\n",
    "        \"EXPL_NOISE\": 0.1,  # Action noise scale\n",
    "        \"MEAN_NOISE\": 0.0,  # Mean action noise\n",
    "        \"THETA\": 0.15,  # Rate of mean reversion in OU noise\n",
    "        \"DT\": 0.01,  # Timestep for OU noise\n",
    "        \"LR_ACTOR\": 0.001,  # Actor learning rate\n",
    "        \"LR_CRITIC\": 0.001,  # Critic learning rate\n",
    "        \"GAMMA\": 0.95,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 100,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "    }\n",
    "\n",
    "    num_envs = 8\n",
    "    # Define the space invaders environment as a parallel environment\n",
    "    env = space_invaders_v2.parallel_env()\n",
    "\n",
    "    # Environment processing for image based observations\n",
    "    env = ss.frame_skip_v0(env, 4)\n",
    "    env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1)\n",
    "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.frame_stack_v1(env, 4)\n",
    "    env = AsyncPettingZooVecEnv([lambda: env for _ in range(num_envs)])\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the multi-agent algo input arguments\n",
    "    try:\n",
    "        state_dim = [env.single_observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.single_observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.single_action_space(agent).n for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "        INIT_HP[\"MAX_ACTION\"] = None\n",
    "        INIT_HP[\"MIN_ACTION\"] = None\n",
    "    except Exception:\n",
    "        action_dim = [env.single_action_space(agent).shape[0] for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "        INIT_HP[\"MAX_ACTION\"] = [\n",
    "            env.single_action_space(agent).high for agent in env.agents\n",
    "        ]\n",
    "        INIT_HP[\"MIN_ACTION\"] = [\n",
    "            env.single_action_space(agent).low for agent in env.agents\n",
    "        ]\n",
    "\n",
    "    # Pre-process image dimensions for pytorch convolutional layers\n",
    "    if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "    INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "\n",
    "    # Create a population ready for evolutionary hyper-parameter optimisation\n",
    "    agent = create_population(\n",
    "        INIT_HP[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        NET_CONFIG,\n",
    "        INIT_HP,\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "        num_envs=num_envs,\n",
    "        device=device,\n",
    "    )[0]\n",
    "\n",
    "    # Configure the multi-agent replay buffer\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "    memory = MultiAgentReplayBuffer(\n",
    "        INIT_HP[\"MEMORY_SIZE\"],\n",
    "        field_names=field_names,\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Define training loop parameters\n",
    "    agent_ids = deepcopy(env.agents)\n",
    "    max_steps = 20000  # Max steps (default: 2000000)\n",
    "    learning_delay = 500  # Steps before starting learning\n",
    "    training_steps = 10000  # Frequency at which we evaluate training score\n",
    "    eval_steps = None  # Evaluation steps per episode - go until done\n",
    "    eval_loop = 1  # Number of evaluation episodes\n",
    "\n",
    "    total_steps = 0\n",
    "\n",
    "    # TRAINING LOOP\n",
    "    print(\"Training...\")\n",
    "    pbar = trange(max_steps, unit=\"step\")\n",
    "    while np.less(agent.steps[-1], max_steps):\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        scores = np.zeros((num_envs, len(agent_ids)))\n",
    "        completed_episode_scores = []\n",
    "        steps = 0\n",
    "        if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "            state = {\n",
    "                agent_id: np.moveaxis(s, [-1], [-3]) for agent_id, s in state.items()\n",
    "            }\n",
    "\n",
    "        for idx_step in range(training_steps // num_envs):\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = agent.get_action(\n",
    "                states=state, training=True, infos=info\n",
    "            )\n",
    "            if agent.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Act in environment\n",
    "            action = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "            next_state, reward, termination, truncation, info = env.step(action)\n",
    "            if not termination:\n",
    "                assert False\n",
    "            scores += np.array(list(reward.values())).transpose()\n",
    "            total_steps += num_envs\n",
    "            steps += num_envs\n",
    "\n",
    "            # Image processing if necessary for the environment\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                next_state = {\n",
    "                    agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                    for agent_id, ns in next_state.items()\n",
    "                }\n",
    "\n",
    "            # Save experiences to replay buffer\n",
    "            memory.save_to_memory(\n",
    "                state,\n",
    "                cont_actions,\n",
    "                reward,\n",
    "                next_state,\n",
    "                termination,\n",
    "                is_vectorised=True,\n",
    "            )\n",
    "\n",
    "            # Learn according to learning frequency\n",
    "            # Handle learn steps > num_envs\n",
    "            if agent.learn_step > num_envs:\n",
    "                learn_step = agent.learn_step // num_envs\n",
    "                if (\n",
    "                    idx_step % learn_step == 0\n",
    "                    and len(memory) >= agent.batch_size\n",
    "                    and memory.counter > learning_delay\n",
    "                ):\n",
    "                    # Sample replay buffer\n",
    "                    experiences = memory.sample(agent.batch_size)\n",
    "                    # Learn according to agent's RL algorithm\n",
    "                    agent.learn(experiences)\n",
    "            # Handle num_envs > learn step; learn multiple times per step in env\n",
    "            elif len(memory) >= agent.batch_size and memory.counter > learning_delay:\n",
    "                for _ in range(num_envs // agent.learn_step):\n",
    "                    # Sample replay buffer\n",
    "                    experiences = memory.sample(agent.batch_size)\n",
    "                    # Learn according to agent's RL algorithm\n",
    "                    agent.learn(experiences)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # Calculate scores and reset noise for finished episodes\n",
    "            reset_noise_indices = []\n",
    "            term_array = np.array(list(termination.values())).transpose()\n",
    "            trunc_array = np.array(list(truncation.values())).transpose()\n",
    "            for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n",
    "                if np.any(d) or np.any(t):\n",
    "                    completed_episode_scores.append(scores[idx])\n",
    "                    agent.scores.append(scores[idx])\n",
    "                    scores[idx] = 0\n",
    "                    reset_noise_indices.append(idx)\n",
    "            agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "        pbar.update(training_steps)\n",
    "\n",
    "        agent.steps[-1] += steps\n",
    "\n",
    "        # Evaluate population\n",
    "        fitness = agent.test(\n",
    "            env,\n",
    "            swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "            max_steps=eval_steps,\n",
    "            loop=eval_loop,\n",
    "            sum_scores=False,\n",
    "        )\n",
    "        pop_episode_scores = np.array(completed_episode_scores)\n",
    "        mean_scores = np.mean(pop_episode_scores, axis=0)\n",
    "\n",
    "        print(f\"--- Global steps {total_steps} ---\")\n",
    "        print(f\"Steps {agent.steps[-1]}\")\n",
    "        print(\"Scores:\")\n",
    "        for idx, sub_agent in enumerate(agent_ids):\n",
    "            print(f\"    {sub_agent} score: {mean_scores[idx]}\")\n",
    "        print(\"Fitness\")\n",
    "        for idx, sub_agent in enumerate(agent_ids):\n",
    "            print(f\"    {sub_agent} fitness: {fitness[idx]}\")\n",
    "        print(\"Previous 5 fitness avgs\")\n",
    "        for idx, sub_agent in enumerate(agent_ids):\n",
    "            print(\n",
    "                f\"  {sub_agent} fitness average: {np.mean(agent.fitness[-5:], axis=0)[idx]}\"\n",
    "            )\n",
    "\n",
    "        # Update step counter\n",
    "        agent.steps.append(agent.steps[-1])\n",
    "\n",
    "    # Save the trained algorithm\n",
    "    path = \"./models/MADDPG\"\n",
    "    filename = \"MADDPG_space_invader_trained_agent.pt\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_path = os.path.join(path, filename)\n",
    "    agent.save_checkpoint(save_path)\n",
    "\n",
    "    pbar.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12122b-44de-400e-a475-36ef5ef77076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
